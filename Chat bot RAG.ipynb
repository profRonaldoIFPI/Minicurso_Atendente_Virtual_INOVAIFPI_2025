{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_qMsPrgGUCW"
   },
   "source": [
    "# **Construindo um Chatbot com a API Gemini, Fundamentado em Documentos de texto**\n",
    "\n",
    "**Além do Conhecimento Geral para uma IA Fundamentada**\n",
    "A tarefa de criar um chatbot que responda a perguntas de usuários é uma aplicação comum de Grandes Modelos de Linguagem (LLMs). No entanto, um desafio significativo com LLMs de propósito geral, como o Gemini, é sua tendência a \"alucinar\".\n",
    "\n",
    "Para aplicações empresariais, acadêmicas ou especializadas, onde a precisão e a rastreabilidade são primordiais, as respostas do chatbot devem ser estritamente limitadas a um conjunto de documentos.\n",
    "\n",
    "A solução para este desafio é uma arquitetura conhecida como **Geração Aumentada por Recuperação (RAG)**. Em vez de simplesmente fazer uma pergunta ao LLM e esperar que ele \"saiba\" a resposta com base em seu vasto treinamento prévio, o RAG introduz um passo intermediário crucial.\n",
    "\n",
    "![](https://static.wixstatic.com/media/cfe500_0546c1c5b8b3430f90c039aaa4ab71e2~mv2.jpg/v1/fill/w_740,h_438,al_c,q_80,usm_0.66_1.00_0.01,enc_avif,quality_auto/cfe500_0546c1c5b8b3430f90c039aaa4ab71e2~mv2.jpg)\n",
    "\n",
    "1. **Prompt do Usuário**: Primeiro, o usuário insere um prompt com a pergunta (query) no sistema.\n",
    "\n",
    "2. **Busca por Informação**: A query é usada para consultar fontes de conhecimento (como documentos PDF, TXT, etc.) previamente indexadas.\n",
    "\n",
    "3. **Recuperação**: O sistema retorna ***trechos relevantes dos documentos*** que servirão como ***contexto aumentado***.\n",
    "\n",
    "4. **Preparação do Prompt**: O contexto recuperado é combinado com a query original, formando um novo prompt fundamentado.\n",
    "\n",
    "5. **Geração da Resposta**: O novo prompt é enviado ao LLM (como o Gemini), que gera uma resposta com base exclusivamente nas informações recuperadas.\n",
    "\n",
    "A arquitetura RAG pode ser definida como um pipeline de três estágios principais: **Indexação, Recuperação e Geração**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOgUukZoJZbn"
   },
   "source": [
    "\n",
    "## 1. **Indexação**\n",
    "\n",
    "A Indexação na pratica é um processo em 3 etapas:\n",
    "\n",
    "1.1 **Ingestão**: Carregar e analisar os arquivos brutos PDF e TXT do seu corpus de conhecimento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUnmY57dLsVa"
   },
   "source": [
    "Instalação das dependencias:\n",
    "* google-generativeai: O SDK oficial do Google para interagir com a API Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6361,
     "status": "ok",
     "timestamp": 1751500941601,
     "user": {
      "displayName": "Ronaldo Pires Borges",
      "userId": "00032249735521270223"
     },
     "user_tz": 180
    },
    "id": "WIR64CKRk_uQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sW3teFxxMFhR"
   },
   "source": [
    "Chave da API do Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2301,
     "status": "ok",
     "timestamp": 1751491853195,
     "user": {
      "displayName": "Ronaldo Pires Borges",
      "userId": "00032249735521270223"
     },
     "user_tz": 180
    },
    "id": "kgC1RH8JMKv7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqjTfgnwOE92"
   },
   "source": [
    " Importando texto de PDF\n",
    "\n",
    " Para este exemplo usaremos a [Organização didática do IFPI](https://www.ifpi.edu.br/acesso-a-informacao/institucional/consuprn1432022organizacaodidatica.pdf).\n",
    "\n",
    " Baixe o arquivo do link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irBspatvlKXF"
   },
   "source": [
    "Instalação das dependencias:\n",
    "* pypdf: Biblioteca para extrair texto de arquivos PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6974,
     "status": "ok",
     "timestamp": 1751500966079,
     "user": {
      "displayName": "Ronaldo Pires Borges",
      "userId": "00032249735521270223"
     },
     "user_tz": 180
    },
    "id": "D1PCL25klJmr"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyiflm-1JvTN"
   },
   "source": [
    "1.2 **Divisão (Chunking)**: Dividir estrategicamente o texto dos documentos em pedaços menores e gerenciáveis, conhecidos como \"chunks\".\n",
    "\n",
    "Esta etapa é fundamental por duas razões principais:\n",
    "\n",
    " - **Limites de Contexto do Modelo**: Os modelos de embedding e os LLMs têm um limite máximo de tokens que podem processar de uma só vez. Enviar um documento inteiro (100 páginas, por exemplo) excederia esse limite.\n",
    "\n",
    "- **Precisão da Recuperação**: Se você incorporar um documento inteiro em um único vetor, o vetor representará o significado médio de todo o documento. Quando um usuário faz uma pergunta específica, é improvável que a média de todo o documento seja a correspondência mais próxima. Chunks menores e mais focados permitem uma recuperação muito mais precisa e relevante.\n",
    "\n",
    "A escolha da estratégia de \"chunking\" é uma decisão de design importante. As estratégias comuns incluem:\n",
    "\n",
    "- **Tamanho Fixo (Fixed-Size)**: A abordagem mais simples, dividindo o texto em chunks de N caracteres ou tokens. Sua principal desvantagem é que pode cortar frases ou parágrafos no meio, quebrando o contexto semântico.\n",
    "- **Semântica (Semantic)**: Tenta dividir o texto em limites lógicos, como frases ou parágrafos.\n",
    "- **Recursiva (Recursive)**: Uma abordagem mais sofisticada que tenta dividir o texto usando uma hierarquia de separadores. Por exemplo, primeiro tenta dividir por parágrafos (\\n\\n). Se os chunks resultantes ainda forem muito grandes, ele os divide por frases, e assim por diante.\n",
    "\n",
    "Para a maioria dos documentos baseados em texto, a **Divisão Recursiva de Caracteres (Recursive Character Text Splitting)** oferece o melhor equilíbrio entre simplicidade e preservação do contexto semântico. Ela respeita a estrutura natural do documento, tentando manter os parágrafos e as frases intactos sempre que possível.\n",
    "\n",
    "Dois parâmetros chave nesta estratégia são `chunk_size` e `chunk_overlap`. `chunk_size` define o tamanho máximo de cada chunk. `chunk_overlap` especifica quantos caracteres do final de um chunk devem ser repetidos no início do próximo.\n",
    "\n",
    "![chunk overlap](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/unnamed-1-67a0e0c9ca199-1.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeZW4bdmlhAb"
   },
   "source": [
    "Instalação das dependencias:\n",
    "* langchain-text-splitters: Uma ferramenta útil para implementar estratégias de divisão de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9048,
     "status": "ok",
     "timestamp": 1751500992358,
     "user": {
      "displayName": "Ronaldo Pires Borges",
      "userId": "00032249735521270223"
     },
     "user_tz": 180
    },
    "id": "BRtdrvr5GMkS"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1751500999070,
     "user": {
      "displayName": "Ronaldo Pires Borges",
      "userId": "00032249735521270223"
     },
     "user_tz": 180
    },
    "id": "cvBCwdS-Gvrw"
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqrVK7lzJ0fn"
   },
   "source": [
    "1.3 **Embedding e Indexação**: Converter cada \"chunk\" de texto em uma representação numérica (um vetor de embedding) e armazenar esses vetores para busca rápida.\n",
    "\n",
    " - *Embedding:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ix7keBLoinCN"
   },
   "source": [
    "- *Indexação:*\n",
    "\n",
    "Os embeddings precisam ser armazenados em um local que permita uma busca por similaridade extremamente rápida. Bancos de dados relacionais tradicionais não são projetados para este tipo de operação. Em vez disso, usamos **armazenamentos de vetores (vector stores)**. Seguem duas opções:\n",
    "1. **[ChromaDB](https://www.trychroma.com/)** é um banco de dados vetorial de código aberto, construído especificamente para aplicações de IA. Ele abstrai grande parte da complexidade. Com uma API simples, ele gerencia o armazenamento de embeddings, documentos e metadados em um único local.  \n",
    "\n",
    "2. **[FAISS (Facebook AI Similarity Search)](https://ai.meta.com/tools/faiss/)** não é um banco de dados, mas sim uma biblioteca de código aberto altamente otimizada para busca de similaridade em conjuntos densos de vetores.\n",
    "\n",
    "Para os propósitos deste guia, que visa construir um protótipo funcional, **ChromaDB** é a escolha recomendada devido à sua simplicidade e ciclo de desenvolvimento rápido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXe8O3A7lqYV"
   },
   "source": [
    "Instalação das dependencias:\n",
    "* ChromaDB: Um banco de dados vetorial de código aberto projetado para IA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5985,
     "status": "ok",
     "timestamp": 1751501044167,
     "user": {
      "displayName": "Ronaldo Pires Borges",
      "userId": "00032249735521270223"
     },
     "user_tz": 180
    },
    "id": "5R8ol8tgimP3"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ylzbtc8dypJg"
   },
   "source": [
    "Adiciona os dados à coleção (como um insert em uma tabela)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhFN6odqJQBc"
   },
   "source": [
    "## 2. **Recuperação**\n",
    "\n",
    "Quando um usuário envia uma consulta, o primeiro passo é encontrar os \"chunks\" de texto mais relevantes em nossa base de conhecimento. Este processo de busca semântica tem duas etapas:\n",
    "\n",
    "1. **Incorporar a Consulta do Usuário**: A consulta do usuário (uma string de texto) deve ser convertida em um vetor de embedding usando o mesmo modelo que usamos para os documentos (text-embedding-004). Crucialmente, aqui usamos `task_type=\"RETRIEVAL_QUERY\"` para otimizar o vetor para a tarefa de busca.\n",
    "\n",
    "2. **Buscar por Similaridade**: O vetor da consulta é então usado para pesquisar no nosso armazenamento de vetores. A \"similaridade\" é tipicamente medida usando a **Similaridade de Cosseno (Cosine Similarity)**. Esta métrica calcula o cosseno do ângulo entre dois vetores. Um valor de `1` significa que os vetores apontam na mesma direção (semanticamente idênticos), `0` significa que são ortogonais (não relacionados), e `-1` significa que são opostos. Matematicamente, é calculado como o produto escalar dos vetores dividido pelo produto de suas magnitudes:\n",
    "$$\n",
    "S_C(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTldKynkJoqb"
   },
   "source": [
    "## 3. **Geração**\n",
    "\n",
    "Nesta etapa vamos usar os \"chunks\" recuperados como contexto para um modelo generativo.\n",
    "\n",
    "Esta é a etapa mais crítica para cumprir a restrição principal do usuário: garantir que o chatbot responda apenas com base nas informações fornecidas. Devemos construir um prompt que instrua o modelo Gemini a abandonar seu conhecimento geral e a se ater estritamente ao contexto que fornecemos. Isso é alcançado através de uma **engenharia de prompt**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suYe8MlD9cVg"
   },
   "source": [
    "**Resposta Final**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBJsjmAvBuM0"
   },
   "source": [
    "**Chat continuo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP9nUknTdWqRcav1qfneVPz",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
