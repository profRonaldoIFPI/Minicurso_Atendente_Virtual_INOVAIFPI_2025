{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_qMsPrgGUCW"
      },
      "source": [
        "# **Construindo um Chatbot com a API Gemini, Fundamentado em Documentos de texto**\n",
        "\n",
        "**Além do Conhecimento Geral para uma IA Fundamentada**\n",
        "A tarefa de criar um chatbot que responda a perguntas de usuários é uma aplicação comum de Grandes Modelos de Linguagem (LLMs). No entanto, um desafio significativo com LLMs de propósito geral, como o Gemini, é sua tendência a \"alucinar\".\n",
        "\n",
        "Para aplicações empresariais, acadêmicas ou especializadas, onde a precisão e a rastreabilidade são primordiais, as respostas do chatbot devem ser estritamente limitadas a um conjunto de documentos.\n",
        "\n",
        "A solução para este desafio é uma arquitetura conhecida como **Geração Aumentada por Recuperação (RAG)**. Em vez de simplesmente fazer uma pergunta ao LLM e esperar que ele \"saiba\" a resposta com base em seu vasto treinamento prévio, o RAG introduz um passo intermediário crucial.\n",
        "\n",
        "![](https://static.wixstatic.com/media/cfe500_0546c1c5b8b3430f90c039aaa4ab71e2~mv2.jpg/v1/fill/w_740,h_438,al_c,q_80,usm_0.66_1.00_0.01,enc_avif,quality_auto/cfe500_0546c1c5b8b3430f90c039aaa4ab71e2~mv2.jpg)\n",
        "\n",
        "1. **Prompt do Usuário**: Primeiro, o usuário insere um prompt com a pergunta (query) no sistema.\n",
        "\n",
        "2. **Busca por Informação**: A query é usada para consultar fontes de conhecimento (como documentos PDF, TXT, etc.) previamente indexadas.\n",
        "\n",
        "3. **Recuperação**: O sistema retorna ***trechos relevantes dos documentos*** que servirão como ***contexto aumentado***.\n",
        "\n",
        "4. **Preparação do Prompt**: O contexto recuperado é combinado com a query original, formando um novo prompt fundamentado.\n",
        "\n",
        "5. **Geração da Resposta**: O novo prompt é enviado ao LLM (como o Gemini), que gera uma resposta com base exclusivamente nas informações recuperadas.\n",
        "\n",
        "A arquitetura RAG pode ser definida como um pipeline de três estágios principais: **Indexação, Recuperação e Geração**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOgUukZoJZbn"
      },
      "source": [
        "\n",
        "## 1. **Indexação**\n",
        "\n",
        "A Indexação na pratica é um processo em 3 etapas:\n",
        "\n",
        "1.1 **Ingestão**: Carregar e analisar os arquivos brutos PDF e TXT do seu corpus de conhecimento.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUnmY57dLsVa"
      },
      "source": [
        "Instalação das dependencias:\n",
        "* google-generativeai: O SDK oficial do Google para interagir com a API Gemini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIR64CKRk_uQ"
      },
      "outputs": [],
      "source": [
        "!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW3teFxxMFhR"
      },
      "source": [
        "Chave da API do Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K52TdSL7olt7"
      },
      "outputs": [],
      "source": [
        "GOOGLE_API_KEY = \"AIzaSyAlh3jMCqGjWCkIc73erVWLjhHUKsli_eU\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2d_qjv2r-JH"
      },
      "outputs": [],
      "source": [
        " # apemas no colab\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY_1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kgC1RH8JMKv7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ronaldo\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqjTfgnwOE92"
      },
      "source": [
        " Importando texto de PDF\n",
        "\n",
        " Baixe os arquivos do GitHub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irBspatvlKXF"
      },
      "source": [
        "Instalação das dependencias:\n",
        "* pypdf: Biblioteca para extrair texto de arquivos PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1PCL25klJmr",
        "outputId": "c3bdc790-356a-4161-ea61-4459319654a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/323.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m317.4/323.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U -q pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AJykavp6oluB"
      },
      "outputs": [],
      "source": [
        "diretorio_arquivo = \"D:/Minicurso_Atendente_Virtual_INOVAIFPI_2025/content\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "l8bNXS6muv1l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pypdf import PdfReader\n",
        "\n",
        "documents = []\n",
        "\n",
        "for filename in os.listdir(diretorio_arquivo):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        pdf_path = os.path.join(diretorio_arquivo, filename)\n",
        "        title = os.path.splitext(filename)[0] #retirar a extensão pdf do nome do arquivo\n",
        "        reader = PdfReader(pdf_path)\n",
        "        pages = reader.pages\n",
        "        doc_text = ''.join((page.extract_text() or '') for page in pages)\n",
        "        documents.append({\"title\": title, \"text\": doc_text})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyiflm-1JvTN"
      },
      "source": [
        "1.2 **Divisão (Chunking)**: Dividir estrategicamente o texto dos documentos em pedaços menores e gerenciáveis, conhecidos como \"chunks\".\n",
        "\n",
        "Esta etapa é fundamental por duas razões principais:\n",
        "\n",
        " - **Limites de Contexto do Modelo**: Os modelos de embedding e os LLMs têm um limite máximo de tokens que podem processar de uma só vez. Enviar um documento inteiro (100 páginas, por exemplo) excederia esse limite.\n",
        "\n",
        "- **Precisão da Recuperação**: Se você incorporar um documento inteiro em um único vetor, o vetor representará o significado médio de todo o documento. Quando um usuário faz uma pergunta específica, é improvável que a média de todo o documento seja a correspondência mais próxima. Chunks menores e mais focados permitem uma recuperação muito mais precisa e relevante.\n",
        "\n",
        "A escolha da estratégia de \"chunking\" é uma decisão de design importante. As estratégias comuns incluem:\n",
        "\n",
        "- **Tamanho Fixo (Fixed-Size)**: A abordagem mais simples, dividindo o texto em chunks de N caracteres ou tokens. Sua principal desvantagem é que pode cortar frases ou parágrafos no meio, quebrando o contexto semântico.\n",
        "- **Semântica (Semantic)**: Tenta dividir o texto em limites lógicos, como frases ou parágrafos.\n",
        "- **Recursiva (Recursive)**: Uma abordagem mais sofisticada que tenta dividir o texto usando uma hierarquia de separadores. Por exemplo, primeiro tenta dividir por parágrafos (\\n\\n). Se os chunks resultantes ainda forem muito grandes, ele os divide por frases, e assim por diante.\n",
        "\n",
        "Para a maioria dos documentos baseados em texto, a **Divisão Recursiva de Caracteres (Recursive Character Text Splitting)** oferece o melhor equilíbrio entre simplicidade e preservação do contexto semântico. Ela respeita a estrutura natural do documento, tentando manter os parágrafos e as frases intactos sempre que possível.\n",
        "\n",
        "Dois parâmetros chave nesta estratégia são `chunk_size` e `chunk_overlap`. `chunk_size` define o tamanho máximo de cada chunk. `chunk_overlap` especifica quantos caracteres do final de um chunk devem ser repetidos no início do próximo.\n",
        "\n",
        "![chunk overlap](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/unnamed-1-67a0e0c9ca199-1.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeZW4bdmlhAb"
      },
      "source": [
        "Instalação das dependencias:\n",
        "* langchain-text-splitters: Uma ferramenta útil para implementar estratégias de divisão de texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "BRtdrvr5GMkS"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cvBCwdS-Gvrw"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "text_spliter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 400, # quantidade caracteres por pedaço\n",
        "    chunk_overlap  = 60,\n",
        "    length_function = len, #função que mede o comprimento do chunk\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \",\", \" \", \"\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ubb73SMq4TQq"
      },
      "outputs": [],
      "source": [
        "chunks = []\n",
        "for doc in documents:\n",
        "  for chunk in text_spliter.split_text(doc[\"text\"]):\n",
        "    chunks.append({\"title\": doc[\"title\"], \"text\": chunk})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SP_IIAV_twX",
        "outputId": "73bcc303-70e3-4375-e2a4-64b5df39a974"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "221"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VTIZF415Z-B",
        "outputId": "3e5c1c17-0b27-460d-e476-dc97dc7b6866"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'title': 'Currículos dos médicos',\n",
              " 'text': 'CURRÍCULOS – CLÍNICA BEM-ESTAR \\n \\nDr. Ricardo Monteiro \\nEspecialidade: Cardiologia \\nCRM: 18234-PI \\nE-mail profissional: ricardo.monteiro@clinicabemestar.com.br \\nFormação Acadêmica: \\n• Graduação em Medicina – Universidade Federal do Piauí (UFPI), 2008 \\n• Residência Médica em Clínica Médica – Hospital Universitário da UFPI, \\n2011'}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QpzeaIA5eza",
        "outputId": "a78c1c3c-517e-4a61-e971-2e6845493d52"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'title': 'Currículos dos médicos',\n",
              " 'text': '2011 \\n• Especialização em Cardiologia – Instituto do Coração (InCor/USP), 2014 \\nExperiência Profissional: \\n• Cardiologista assistente no Hospital São Marcos (2015–2020) \\n• Coordenador do setor de cardiologia preventiva na Clínica Bem-Estar \\n(desde 2021) \\nÁreas de interesse: prevenção de doenças cardiovasculares, hipertensão arterial \\ne reabilitação cardíaca. \\nIdiomas: Português e Inglês técnico.'}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqrVK7lzJ0fn"
      },
      "source": [
        "1.3 **Embedding e Indexação**: Converter cada \"chunk\" de texto em uma representação numérica (um vetor de embedding) e armazenar esses vetores para busca rápida.\n",
        "\n",
        " - *Embedding:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZeVHjXdhAk4S"
      },
      "outputs": [],
      "source": [
        "models_embeddings = []\n",
        "for model in genai.list_models():\n",
        "  if \"embedContent\" in model.supported_generation_methods:\n",
        "    models_embeddings.append(model.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI1GHynFoluD",
        "outputId": "aa3956a2-a692-40f6-f5fb-128fd61bed44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['models/embedding-001',\n",
              " 'models/text-embedding-004',\n",
              " 'models/gemini-embedding-exp-03-07',\n",
              " 'models/gemini-embedding-exp',\n",
              " 'models/gemini-embedding-001']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "models_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDkS2576DbvI"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"title\": \"nome do arquivo\",\n",
        "  \"text\": \"texto...\",\n",
        "  \"embedding\": \"0.56, 0.455...\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "rRe74p7kCJHu",
        "outputId": "89abfb6f-fee4-44ec-bd72-727f6e998746"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 172 ms\n",
            "Wall time: 1min 36s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "embeddedChunks = []\n",
        "for chunk in chunks:\n",
        "  try:\n",
        "    result = genai.embed_content(\n",
        "        model = models_embeddings[1],\n",
        "        content = chunk[\"text\"],\n",
        "        task_type=\"RETRIEVAL_DOCUMENT\"\n",
        "    )\n",
        "    chunk[\"embedding\"] = result[\"embedding\"]\n",
        "    embeddedChunks.append(chunk)\n",
        "  except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJlrQrZHEphL",
        "outputId": "e241c183-f602-4b90-97aa-83010b2584ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.015707677, -0.0060636885, -0.03130821, 0.03827866, 0.0351655, 0.0052398075, -0.008128903, 0.0014418159, 0.018109782, 0.1008451, -0.01283055, 0.08683725, 0.024728663, 0.01889299, -0.0020345708, -0.02195822, 0.020604488, -0.0077352566, -0.1018801, 0.022693759, 0.03788595, -0.024486007, 0.018751936, -0.0056941207, -0.0073814737, -0.020754224, 0.064993285, -0.049349606, -0.048890803, 0.004154435, 0.02525067, 0.020227294, -0.0066437647, -0.036102194, 0.02611773, 0.07603179, 0.013023253, -0.0056571206, 0.0018348971, -0.055431288, -0.035242327, 0.004870376, 0.0051701525, 0.0053727413, -0.030761804, -0.058892146, -0.01643943, 0.05696354, -0.06541251, 0.045866717, 0.0138657885, 0.06800581, -0.022755051, 0.052283846, -0.023550807, -0.027739897, -0.065049104, -0.04586999, 0.030112196, -0.014733079, -0.0050789206, -0.0035487649, 0.011329759, -0.072480604, 0.057893578, 0.0009906903, -0.0062918216, -0.0039049122, -0.073337995, 0.004451665, 0.011417545, 0.025442332, 0.01151085, 0.005427833, -0.01865086, -0.01931488, 0.021863665, -0.03733964, 0.03985979, 0.018400494, -0.07890103, -0.03349591, 0.034724902, 0.07060313, -0.003975522, 0.04473745, 0.07162621, -0.04021775, -0.00071922084, 0.05337104, 0.15737271, 0.029034592, 0.011174314, -0.014229806, 0.022253668, 0.022354975, -0.1021552, -0.06789184, 0.07097823, 0.004293148, 0.04933178, 0.020684224, 0.023763867, 0.009258012, 0.0105435895, 0.017812315, 0.035952058, -0.016858779, 0.004422936, 0.056097705, 0.019629406, -0.0040557384, 0.033313494, 0.014855105, -0.042335674, 0.007060655, -0.006847251, 0.005857806, 0.01144071, 0.0431487, -0.007908664, 0.034254804, -0.08317615, -0.00391954, 0.025457721, 0.0037010417, -0.025894698, -0.047483586, -0.012208402, -0.03250773, 0.047282677, -0.050534617, -0.02956397, -0.035275828, -0.01025199, 0.022973493, 0.064098515, -0.020006262, 0.004711179, 0.042166468, 0.0020658658, -0.07404516, -0.03974133, 0.0010493197, 0.002065872, -0.042188864, 0.023461998, 0.026128847, -0.015665492, -0.028658085, -0.0008017351, -0.028704375, 0.057732727, -0.0058945273, -0.000858048, -0.055394493, 0.038917527, -0.031020233, 0.008115951, 0.022329073, 0.030430239, -0.0114536155, 0.039613802, 0.041698, -0.06761481, 0.030774087, 0.021408742, -0.0066692876, -0.011889894, -0.019709451, 0.0024939259, -0.050049335, 0.03648609, -0.072562285, 0.012333259, 0.01653261, 0.0031861737, 0.0013813449, 0.015065332, 0.025845272, 0.12728016, 0.007490682, 0.02273859, -0.028045626, -0.036820654, -0.0008012521, 0.07898061, 0.02555955, 0.07620159, 0.04535726, -0.04024904, 0.03655436, 0.044270977, 0.03123677, -0.03612942, 0.013631967, -0.022510746, -0.021001717, 0.052191485, 0.009059543, 0.012667536, 0.09378235, 0.0057313317, -0.04094622, -0.014924729, 0.035835735, -0.076563925, 0.020460593, -0.043802306, 0.012573932, 0.031643532, 0.01677533, -0.084101014, -0.047627464, -0.016053699, 0.011256132, 0.057459176, -0.009924845, 0.005680658, -0.03393987, 0.020999981, -0.0005021747, 0.02389299, 0.034876067, 0.030328516, -0.008881001, -0.04830294, -0.042330153, 0.014116851, -0.026263082, -0.041630197, -0.02848784, 0.018363614, 0.011054683, -0.020899847, -0.028024577, 0.064130954, -0.044728957, 0.010858894, -0.06863832, 0.038199134, 0.034524005, 0.06287189, 0.0010384632, 0.07818604, 0.017886443, 0.084888525, 0.0031250643, 0.003695905, -0.051563315, -0.08007042, 0.028849453, -0.0015347027, -0.01224582, -0.062249295, -0.06035194, -0.036071595, 0.004425525, -0.015042922, -0.0130450325, 0.027569575, 0.00026196244, 0.009493263, -0.074440174, -0.02313293, -0.072129555, -0.018573932, 0.02055961, 0.054457977, 0.0062729525, 0.010029823, -0.016446568, 0.0033793652, -0.030758703, -0.02726963, 0.02628522, 0.03297717, -0.010777804, -0.042873565, -0.027565068, 0.07997975, -0.0022311644, -0.010337906, -0.005897364, 0.017586786, -0.058391448, 0.034562707, 0.023883952, 0.010729307, -0.04603076, -0.021410512, 0.022423953, -0.082940266, -0.008190266, 0.00326895, -0.0073818774, 0.024054673, 0.043444708, 0.01849534, 0.005921329, -0.029940406, 0.02359091, -0.031039754, -0.0025482716, 0.0070679905, -0.02299694, -0.016442085, 0.00025982907, -0.02175702, 0.040431205, 0.0043781493, 0.025968459, -0.03810979, -0.043281205, -0.014036734, -0.0283921, -0.13943602, 0.03444289, 0.047027916, 0.004660068, -0.001429494, 0.04906229, 0.016365107, 0.023435354, -0.048379168, 0.015619461, 0.01057068, 0.018866358, 0.044555716, 0.008145069, 0.03272895, -0.026416559, -0.026694603, -0.06518233, 0.00069957186, -0.0033775182, -0.07527445, 0.016651206, 0.016853929, 0.0697896, 0.002561193, 0.031119615, 0.011108604, 0.028437134, 0.0010573955, -0.04801051, -0.027785962, 0.0011725951, -0.0030076862, -0.039210796, 0.012354373, 0.051544927, -0.0100704925, -0.04960055, -0.031391192, -0.01659473, 0.0066279084, 0.020728461, 0.037615363, 0.0012399983, -0.049887754, 0.04295523, -0.014906785, 0.0104484875, 0.0006014758, 0.004078007, 0.038279112, 0.04213492, -0.0006969737, -0.0036187705, -0.025017029, -0.0014760447, 0.030169321, 0.021510808, -0.0022371341, 0.002256876, 0.023497263, -0.0016411612, -0.0035729187, -0.011216261, 0.008611346, -0.01861597, -0.0058182264, -0.0024804901, 0.036882155, 0.05206034, -0.05239303, 0.008654182, -0.012428986, -0.005191056, 0.0063165952, 0.009582859, 0.018526023, 0.026156167, -0.0304928, 0.082967624, 0.047434658, -0.013572907, 0.020665681, -0.026859054, 0.04329239, -0.044844635, 0.05289163, -0.02510627, -0.02715168, 0.004257718, 0.036478315, 0.032145932, 0.048570283, 0.025454037, 0.009045788, 0.0206166, -0.02543702, -0.019601764, -0.05390632, -0.018713163, 0.032534063, -0.02906411, 0.036526904, 0.055358358, -0.020481855, 0.061909102, 0.01567251, 0.017662574, -0.0054891733, -0.067203246, -0.01308413, -0.041945767, 0.013899133, -0.036333546, 0.026430963, 0.013535455, -0.035842262, 0.06383547, 0.060967628, 0.03846642, 0.03223541, -0.036611162, -0.05557096, 0.036838785, -0.03594081, 0.012175009, 0.043378223, 0.0125571, -0.028454693, 0.029191084, 0.060344774, 0.021675427, 0.027476901, -0.02833213, -0.0020559074, -0.02774688, 0.004825142, 0.021044008, -0.029081127, 0.0042677945, -0.009522937, 0.018272541, -0.045603592, -0.023413857, -0.043500666, -0.025093269, -0.0065827714, -0.0067998166, -0.01802806, 0.023665093, -0.0001755565, -0.019641325, -0.0026978478, 0.050685227, 0.040305004, -0.0047556637, 0.035847407, 0.014092734, -0.013591697, -0.03676311, 9.672396e-05, 0.023006363, -0.0074724085, 0.042091552, 0.0038049181, 0.031838216, -0.040770054, 0.01949843, 0.055176508, -0.021085417, -0.03331806, -0.00012726277, 0.010408934, 0.063250475, 0.026320059, 0.018176226, -0.011464526, 9.415833e-05, -0.025665743, -0.0036017776, 0.02637826, -0.04571505, -0.017663848, 0.07064079, 0.0854706, 0.002428772, 0.02031873, -0.084495455, -0.022445155, 0.01085545, -0.014138929, 0.006232947, 0.03596729, -0.014143966, 0.018588152, 0.04606443, 0.07559523, -0.024204386, -0.02397429, 0.033001184, 0.014815547, 0.021345908, -0.012767458, 0.019520277, -0.0018261661, -0.0027659119, -0.0036254344, -0.027207226, 0.02733925, 0.00028193244, 0.04289555, -0.05064187, 0.028539771, -0.0835439, -0.016525358, 0.076901175, 0.05417197, -0.013633785, 0.012129945, 0.056845386, 0.046792056, -0.027508266, -0.022317056, -0.08747249, -0.0094826035, -0.03320889, -0.018434016, 0.047910936, 0.014009116, 0.026176114, -0.021069113, 0.030372411, -0.0064422935, 0.12280188, 0.021998193, -0.026483722, -0.028545422, 0.0152826905, 0.008454839, -0.05319923, 0.07196846, -0.00092432287, -0.04374213, -0.06241355, -0.029381126, -0.017819725, 0.0035468976, -0.0015882275, 0.0052000945, 0.032415938, -0.0022136616, 0.008144857, 0.0030951381, 0.025686063, 0.022721792, -0.023722269, 0.04714293, -0.07224441, 0.025571337, -0.039986607, -0.072464965, 0.01440855, 0.0040343283, -0.019788612, -0.02203389, 0.013539525, 0.04726148, 0.037296552, -0.005017959, -0.047461264, 0.024422098, 0.018152706, -0.040908653, -0.009207977, -0.036338326, 0.030124672, 0.013311367, -0.0746033, 0.025600266, -0.003314423, -0.025468728, -0.008656294, -0.024328751, 0.015506371, -0.045490496, 0.04464662, 0.05682112, -0.025675898, -0.021148166, -0.09545836, 0.020842707, -0.0719726, 0.016360944, -0.03955115, -0.0041940734, -0.016587824, -0.06320916, 0.019828148, 0.005654142, -0.05623244, -0.0046440605, -0.055526573, -0.035279334, 0.018493405, -0.022508286, -0.0040776683, 0.01853002, 0.009746684, 0.010012261, 0.032850754, -0.023634223, 0.013823168, 0.013607417, -0.026858516, 0.012890786, 0.009069649, 0.041338444, 0.0049546175, -0.005267083, 0.04895718, -0.06001552, -0.0015334116, 0.029553503, -0.050761353, 0.008995092, 0.012539315, 0.02862163, 0.04854196, -0.0394417, 0.03600479, 0.025981475, -0.061847065, 0.02931608, -0.002557798, 0.02670055, 0.022912854, -0.024077935, 0.02003855, 0.011830408, -0.012465284, -0.027744638, -0.029922418, 0.015255269, -0.025763799, 0.006171143, 0.00020962875, 0.031836763, 0.02526489, 0.0148757035, -0.042792883, -0.0054864474, -0.014655983, -0.012363146, -0.038928267, -0.017573228, -0.04592577, 0.013344487, -0.016600229, -0.011810885, -0.0049185413, 0.00824031, 0.030657591, -0.0028077578, -0.035413887, -0.0020469485, -0.036668077, -0.008748952, -0.053548485, 0.006725653, 0.030084128, 0.054645978, 0.017653422, -0.039039657, 0.0008005673, 0.06893713, -0.11307327, -0.026824282, 0.03404137, -0.024674803, 0.06923814, 0.01953997, 0.004849847, 0.016563924, -0.038230967, -0.022278953, -0.040219426, -0.03048816, 0.0181282, 0.01150221, -0.03992499, 0.06191546, 0.042071838, -0.03058954, -0.019164318, -0.027050804, 0.0048013846, -0.032377917, 0.01850176, 0.027692748, 0.014345971, 0.044544138, 0.0068686763, -0.03084929, 0.0639677, -0.017844087, 0.018000422, 0.036486126, 0.014548012, 0.052782387, -0.013997515, -0.06478246, -0.04233477, -0.041558005, 0.059686083, 0.014302026, 0.051854458, -0.020537717, -0.049886484, -0.0056103077, -0.026419576, 0.00033573122, -0.025713895, -0.014159103, -0.025842072, 0.03019991, -0.027934572, -0.007025951, 0.020400552, 0.03266055, 0.008462399, -0.0022385467, 0.011124043, -0.0107597485, -0.0019170853, -0.054072276, -0.016399177, -0.024026524, 0.023620322, -0.021692531, 0.02902125, -0.04451997, -0.018124737, 0.015839117, 0.027209839, -0.011505143, 0.08692009, 0.011974311, 0.0024693715, -0.031098584, -0.022549052, -0.0060770707, 0.016126605]\n"
          ]
        }
      ],
      "source": [
        "print(embeddedChunks[0]['embedding'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix7keBLoinCN"
      },
      "source": [
        "- *Indexação:*\n",
        "\n",
        "Os embeddings precisam ser armazenados em um local que permita uma busca por similaridade extremamente rápida. Bancos de dados relacionais tradicionais não são projetados para este tipo de operação. Em vez disso, usamos **armazenamentos de vetores (vector stores)**. Seguem duas opções:\n",
        "1. **[ChromaDB](https://www.trychroma.com/)** é um banco de dados vetorial de código aberto, construído especificamente para aplicações de IA. Ele abstrai grande parte da complexidade. Com uma API simples, ele gerencia o armazenamento de embeddings, documentos e metadados em um único local.  \n",
        "\n",
        "2. **[FAISS (Facebook AI Similarity Search)](https://ai.meta.com/tools/faiss/)** não é um banco de dados, mas sim uma biblioteca de código aberto altamente otimizada para busca de similaridade em conjuntos densos de vetores.\n",
        "\n",
        "Para os propósitos deste guia, que visa construir um protótipo funcional, **ChromaDB** é a escolha recomendada devido à sua simplicidade e ciclo de desenvolvimento rápido."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXe8O3A7lqYV"
      },
      "source": [
        "Instalação das dependencias:\n",
        "* ChromaDB: Um banco de dados vetorial de código aberto projetado para IA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R8ol8tgimP3",
        "outputId": "860b3c2a-28fe-490d-a1b4-342e65df5168"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q chromadb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylzbtc8dypJg"
      },
      "source": [
        "Adiciona os dados à coleção (como um insert em uma tabela)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "amti7l5RoluF"
      },
      "outputs": [],
      "source": [
        "import chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DfqSGwD6G-5n"
      },
      "outputs": [],
      "source": [
        "DB_PATH = \"./chroma_db\"\n",
        "COLLECTION_NAME = \"content_chatbot\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5FwpFuPJ-8V"
      },
      "source": [
        "Criando/Organizando os dados no formato do chormadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "utgtSzfGG_0w"
      },
      "outputs": [],
      "source": [
        "ids = [f'chunk_{i}' for i in range(len(embeddedChunks))]\n",
        "embeddings = [chunk['embedding'] for chunk in embeddedChunks]\n",
        "documents = [chunk['text'] for chunk in embeddedChunks]\n",
        "metadatas = [{'title': chunk['title']} for chunk in embeddedChunks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l3GQ7iOKGz9"
      },
      "source": [
        "Inserindo os dados no chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XnEVo5NeKxmx"
      },
      "outputs": [],
      "source": [
        "client = chromadb.PersistentClient(path=DB_PATH)\n",
        "collection = client.get_or_create_collection(name=COLLECTION_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpKUzEIGJ54q",
        "outputId": "4d19555f-59ff-4d8c-b2d8-d2ab044693b9"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  collection.upsert(\n",
        "      ids=ids,\n",
        "      embeddings=embeddings,\n",
        "      documents=documents,\n",
        "      metadatas=metadatas\n",
        "  )\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhFN6odqJQBc"
      },
      "source": [
        "## 2. **Recuperação**\n",
        "\n",
        "Quando um usuário envia uma consulta, o primeiro passo é encontrar os \"chunks\" de texto mais relevantes em nossa base de conhecimento. Este processo de busca semântica tem duas etapas:\n",
        "\n",
        "1. **Incorporar a Consulta do Usuário**: A consulta do usuário (uma string de texto) deve ser convertida em um vetor de embedding usando o mesmo modelo que usamos para os documentos (text-embedding-004). Crucialmente, aqui usamos `task_type=\"RETRIEVAL_QUERY\"` para otimizar o vetor para a tarefa de busca.\n",
        "\n",
        "2. **Buscar por Similaridade**: O vetor da consulta é então usado para pesquisar no nosso armazenamento de vetores. A \"similaridade\" é tipicamente medida usando a **Similaridade de Cosseno (Cosine Similarity)**. Esta métrica calcula o cosseno do ângulo entre dois vetores. Um valor de `1` significa que os vetores apontam na mesma direção (semanticamente idênticos), `0` significa que são ortogonais (não relacionados), e `-1` significa que são opostos. Matematicamente, é calculado como o produto escalar dos vetores dividido pelo produto de suas magnitudes:\n",
        "$$\n",
        "S_C(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bRpzFfBjoluF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'models/text-embedding-004'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "models_embeddings[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "def recupera_chunks_relevantes(query: str, n_resultados: int = 5) -> List[str]:\n",
        "    query_embedding_result = genai.embed_content(\n",
        "        model=models_embeddings[1],\n",
        "        content=query,\n",
        "        task_type=\"RETRIEVAL_QUERY\"\n",
        "    )\n",
        "    query_embedding = query_embedding_result['embedding']\n",
        "    result = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=n_resultados\n",
        "    )\n",
        "    return result['documents']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['2. Horário de Funcionamento \\n• Segunda a Sexta: 7h às 19h \\n• Sábado: 7h às 12h \\n• Domingos e feriados: fechado \\nO atendimento na recepção inicia às 6h45. Consultas agendadas têm tolerância \\nde 15 minutos. Após esse período, o horário poderá ser realocado. \\n \\n3. Especialidades e Corpo Clínico \\nMédico Especialidade Dias de Atendimento Horário \\nDr. Ricardo Monteiro Cardiologia Segunda, Quarta,',\n",
              "  'Dr. Ricardo Monteiro Cardiologia Segunda, Quarta, \\nSexta 8h – 12h \\nDra. Helena Prado Pediatria Segunda a Quinta 13h – 17h \\nDr. Gustavo Lemos Ortopedia Terça e Quinta 8h – \\n11h30 \\nDra. Laura Martins Ginecologia e \\nObstetrícia Segunda e Quarta 14h – 18h \\nDr. André \\nVasconcelos Dermatologia Terça e Sexta 9h – 13h \\nDra. Camila Ribeiro Endocrinologia Quarta e Sexta 8h – 12h',\n",
              "  'CLÍNICA BEM-ESTAR – MANUAL DE FUNCIONAMENTO \\n1. Apresentação \\nA Clínica Bem-Estar é um centro de saúde multidisciplinar voltado ao \\natendimento ambulatorial de pacientes particulares, conveniados a planos de \\nsaúde e usuários do SUS. Nosso foco é oferecer atendimento humanizado, com \\nagilidade e qualidade técnica em diversas especialidades médicas. \\n \\n2. Horário de Funcionamento',\n",
              "  'Pacientes atrasados acima de 15 minutos podem perder o atendimento. \\n• Privacidade: Todos os dados e históricos médicos são armazenados \\nconforme a LGPD (Lei Geral de Proteção de Dados). \\n• Emergências: Casos de urgência são encaminhados para o hospital \\nconveniado mais próximo. \\n• Comunicação: A clínica não fornece diagnósticos por telefone ou redes \\nsociais.',\n",
              "  'dias úteis;\\n• Internação – em até 21 dias úteis;\\n• Urgência/Emergência (determinada pelo médico assistente) - atendimento \\nimediato.Dicas úteis para quem tem ou deseja ter um plano de saúde 9Dicas úteis para quem tem ou deseja ter um plano de saúde 9\\nSaiba os prazos máximos para outros serviços em www.ans.gov.br.\\nMeu médico só tem disponibilidade para consulta daqui a um mês.']]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retorno = recupera_chunks_relevantes(\"horario\")\n",
        "retorno"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTldKynkJoqb"
      },
      "source": [
        "## 3. **Geração**\n",
        "\n",
        "Nesta etapa vamos usar os \"chunks\" recuperados como contexto para um modelo generativo.\n",
        "\n",
        "Esta é a etapa mais crítica para cumprir a restrição principal do usuário: garantir que o chatbot responda apenas com base nas informações fornecidas. Devemos construir um prompt que instrua o modelo Gemini a abandonar seu conhecimento geral e a se ater estritamente ao contexto que fornecemos. Isso é alcançado através de uma **engenharia de prompt**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHCs7QPAoluF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suYe8MlD9cVg"
      },
      "source": [
        "**Resposta Final**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "k3E7M2EgoluG"
      },
      "outputs": [],
      "source": [
        "from google.generativeai.types import HarmCategory, HarmBlockThreshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 - models/gemini-2.5-pro-preview-03-25\n",
            "1 - models/gemini-2.5-flash-preview-05-20\n",
            "2 - models/gemini-2.5-flash\n",
            "3 - models/gemini-2.5-flash-lite-preview-06-17\n",
            "4 - models/gemini-2.5-pro-preview-05-06\n",
            "5 - models/gemini-2.5-pro-preview-06-05\n",
            "6 - models/gemini-2.5-pro\n",
            "7 - models/gemini-2.0-flash-exp\n",
            "8 - models/gemini-2.0-flash\n",
            "9 - models/gemini-2.0-flash-001\n",
            "10 - models/gemini-2.0-flash-exp-image-generation\n",
            "11 - models/gemini-2.0-flash-lite-001\n",
            "12 - models/gemini-2.0-flash-lite\n",
            "13 - models/gemini-2.0-flash-preview-image-generation\n",
            "14 - models/gemini-2.0-flash-lite-preview-02-05\n",
            "15 - models/gemini-2.0-flash-lite-preview\n",
            "16 - models/gemini-2.0-pro-exp\n",
            "17 - models/gemini-2.0-pro-exp-02-05\n",
            "18 - models/gemini-exp-1206\n",
            "19 - models/gemini-2.0-flash-thinking-exp-01-21\n",
            "20 - models/gemini-2.0-flash-thinking-exp\n",
            "21 - models/gemini-2.0-flash-thinking-exp-1219\n",
            "22 - models/gemini-2.5-flash-preview-tts\n",
            "23 - models/gemini-2.5-pro-preview-tts\n",
            "24 - models/learnlm-2.0-flash-experimental\n",
            "25 - models/gemma-3-1b-it\n",
            "26 - models/gemma-3-4b-it\n",
            "27 - models/gemma-3-12b-it\n",
            "28 - models/gemma-3-27b-it\n",
            "29 - models/gemma-3n-e4b-it\n",
            "30 - models/gemma-3n-e2b-it\n",
            "31 - models/gemini-flash-latest\n",
            "32 - models/gemini-flash-lite-latest\n",
            "33 - models/gemini-pro-latest\n",
            "34 - models/gemini-2.5-flash-lite\n",
            "35 - models/gemini-2.5-flash-image-preview\n",
            "36 - models/gemini-2.5-flash-image\n",
            "37 - models/gemini-2.5-flash-preview-09-2025\n",
            "38 - models/gemini-2.5-flash-lite-preview-09-2025\n",
            "39 - models/gemini-robotics-er-1.5-preview\n",
            "40 - models/gemini-2.5-computer-use-preview-10-2025\n"
          ]
        }
      ],
      "source": [
        "list_model = []\n",
        "for i, model in enumerate(genai.list_models()):\n",
        "    if 'generateContent' in model.supported_generation_methods:\n",
        "        print(f'{i-1} - {model.name}')\n",
        "        list_model.append(model.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "generation_config = {\n",
        "    \"candidate_count\": 1,\n",
        "    \"temperature\": 1\n",
        "}\n",
        "# safety_settings = {\n",
        "#     HarmCategory.HARM_CATEGORY_HARASSMENT: ...\n",
        "#     ...\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel(\n",
        "    model_name=list_model[2],\n",
        "    generation_config=generation_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sou um modelo de linguagem grande, treinado pelo Google.\n"
          ]
        }
      ],
      "source": [
        "prompt = input()\n",
        "resposta = model.generate_content(prompt)\n",
        "print(resposta.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBJsjmAvBuM0"
      },
      "source": [
        "**Chat continuo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC-_0u8holuG"
      },
      "outputs": [],
      "source": [
        "chat = model.start_chat(history=[])\n",
        "\n",
        "prompt = ''\n",
        "while prompt!=\"fim\":\n",
        "    prompt = input()\n",
        "    resposta = chat.send_message(prompt)\n",
        "    print(resposta.text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
